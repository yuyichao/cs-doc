<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>16.3. Request Processing</title>
<link rel="STYLESHEET" type="text/css" href="images/style.css">
<link rel="STYLESHEET" type="text/css" href="images/docsafari.css">
</head>
<body >
<head>
<link rel="stylesheet" type="text/css" href="../style/visited-green.css">
</head>
<div align=center>
<script type="text/javascript" src="http://j.maxmind.com/app/geoip.js"></script>
<center>
<table width=100% height=90>
<tr style='display:none'>
<td style='display:xnone' id='banner' xalign=center style="background-image:url(/kernel_map.d/LKM3_2048.png);width:100%;height:90;opacity:0;filter:alpha(opacity=0);
cursor:pointer" onclick="top.location='http://www.makelinux.com/kernel_map?b'" /> 
</td> </tr>
<tr style='display:none'>
<td  id='banner2' style="opacity:0;filter:alpha(opacity=0);text-align:center;" /> 
<a target=_top href=http://www.makelinux.com/kernel_map_poster?b>
<span style="font-weight:bold"><span style="display:block;font-size:large" >Poster of Linux kernel</span>The best gift for a Linux geek</span>
</a>
</td> </tr>
<tr style='zdisplay:xnone' >
<td  id='banner3' align=center /> 
<a Xtarget=_top href="http://www.makelinux.com/kernel_map_poster?b"> <img target=_top src="http://www.makelinux.net/kernel_map.d/poster2.png" border=0></a>
</td></tr>
</table>
</center>
<script type='text/javascript' src='../common/fade.js'></script>
<script type=text/javascript>

	var banner = document.getElementById('banner');
	banner.style.backgroundPosition="50% 50%";
	banner.style.backgroundPosition=100*Math.random()+"% "+100*Math.random(100)+"%";
	//fade('banner');
    	//setTimeout("fade('banner')",1000);
    	//setTimeout("fade('banner2')",1000);
</script>

<script type="text/javascript">
var a = new Array();
a[0]='<a href=http://www.linuxdriver.co.il/>www.LinuxDriver.co.il - Embedded Linux solutions: Drivers, Media Streaming, Fast Boot. In Tel-Aviv</a>';
a[1]='<a href=http://www.MakeLinux.net/>www.MakeLinux.net - Embedded Linux solutions: Drivers, Media Streaming, Fast Boot</a>';
a[2]='<a href="http://www.amazon.com/gp/product/0672329468?ie=UTF8&tag=makelinux-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0672329468">New book <b>Linux Kernel Development</b> (3rd Edition) 2010</a><img src="http://www.assoc-amazon.com/e/ir?t=makelinux-20&l=as2&o=1&a=0672329468" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />';
a[3]='';
google_ad_width = 728;
google_ad_height = 1;
if ( 0) { 
document.write("<center>");
if (  geoip_country_code()=="IL" ) {
	if ( Math.random() > 0.5 ) document.write(a[0]+"<br>"); else document.write(a[2]);;
	google_ad_width = 728;
	google_ad_height = 1;
} else { 
	if ( Math.random() > 0.5 ) {
		if ( Math.random() > 0.5 ) document.write(a[1]+"<br>"); else document.write(a[2]);
		google_ad_width = 728;
		google_ad_height = 1;
	} else {
		google_ad_client = "pub-5656623102424572";
		/* 728x90, created 4/4/08 */
		google_ad_slot = "6613964975";
		google_ad_width = 728;
		google_ad_height = 100;
	}
}
	document.write("</center>");
}
</script>
<!--
<script type="text/javascript" src="http://pagead2.googlesyndication.com/pagead/show_ads.js"> </script>
-->


</div>
<!-- 
Hi surfer
<script type="text/javascript"> 
	try {
	document.write(" from <i><b>" + geoip_city()+"</b></i>"); 
	} catch (e) {
	}
</script>, please visit -->


<xhr>
<script type="text/javascript">
	s = document.location.href.lastIndexOf("/");
	a = document.location.href.substring(0,s+1);
	b = document.location.href.substring(s+1);
if ( document.location == top.location  ) {
	//alert(a + " -- " + b);
	document.write("<a href="http://www.makelinux.net/ldd3/+&#32;a&#32;+"?u=" +b + "> &lt; open Table of Content</a>");
	//top.location = a + "?u=" +b;
} else {
	document.write("<a target=_top href="http://www.makelinux.net/ldd3/+document.location&#32;+"> &lt; full page </a>");
}
function addLoadEvent(func) 
{
	var oldonload = window.onload;
	if (typeof window.onload != 'function') {
		window.onload = func;
	} else {
		window.onload = function() {
			oldonload();
			func();
		}
	}
}

addLoadEvent(  function() { 
		try {
		} catch (e) {
	}
}
);


</script>
<span style='display:none;background:#BBFFFF;color:black;position:absolute;right:0;' ><a target=_main href=http://www.makelinux.net/kernel_map?src=ldd3>&nbsp;Linux kernel map&nbsp;</a></span><br>
<script type="text/javascript">
</script>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript"> </script>
<script type="text/javascript"> _uacct = "UA-839593-1"; if (typeof(urchinTracker) == 'function') urchinTracker();</script>
<script type=text/javascript>
	// document.write("<img src=http://const.homelinux.net/1.png?REF="+top.document.referrer+" height=0 width=0 border=0 />");
</script>


<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr >
<td class="v2" align="left" width="30%">
<a href="chp-16-sect-2.shtml"> &#8678; prev </a>
</td>
<td class="v2" align="center" width="40%">
<a href="index.html" target=_parent style="text-decoration:none;text-underline:none"> &#8689; home </a>
</td>
<td class="v2" align="right" width="30%">
<a href="chp-16-sect-4.shtml"> next &#8680; </a>
</td>
</tr>
</table>
<br>
<table width="100%" border="0" cellspacing="0" cellpadding="0"><TR><td valign="top"><a name="chp-16-sect-3"></a>
<h3 class="docSection1Title" >16.3. Request Processing</h3>

<p class="docText">The core of every block driver<a name="chp-16-ITERM-7361"></a> <a name="chp-16-ITERM-7362"></a> <a name="chp-16-ITERM-7363"></a>
<a name="chp-16-ITERM-7364"></a> is its
<span class="docEmphasis">request</span> function. This function is where the
real work gets done—or at least started; all the rest is
overhead. Consequently, we spend a fair amount of time looking at
request processing in block drivers.</p>

<p class="docText">A disk driver's performance can be a critical part
of the performance of the system as a whole. Therefore, the
kernel's block subsystem has been written with
performance very much in mind; it does everything possible to enable
your driver to get the most out of the devices it controls. This is a
good thing, in that it enables blindingly fast I/O. On the other
hand, the block subsystem unnecessarily exposes a great deal of
complexity in the driver API. It is possible to write a very simple
<span class="docEmphasis">request</span> function (we will see one shortly), but
if your driver must perform at a high level on complex hardware, it
will be anything but simple.</p>

<a name="chp-16-sect-3.1"></a>
<h4 class="docSection2Title">16.3.1. Introduction to the request Method</h4>

<p class="docText">The block driver <span class="docEmphasis">request</span> method has the
following prototype:</p>

<pre>void request(request_queue_t *queue);</pre><br>


<p class="docText">This function is called whenever the kernel believes it is time for
your driver to process some reads, writes, or other operations on the
device. The <span class="docEmphasis">request</span> function does not need to
actually complete all of the requests on the queue before it returns;
indeed, it probably does not complete any of them for most real
devices. It must, however, make a start on those requests and ensure
that they are all, eventually, processed by the driver.</p>

<p class="docText">Every device has a request queue. This is because actual transfers to
and from a disk can take place far away from the time the kernel
requests them, and because the kernel needs the flexibility to
schedule each transfer at the most propitious moment (grouping
together, for instance, requests that affect sectors close together
on the disk). And the <span class="docEmphasis">request</span> function, you may
remember, is associated with a request queue when that queue is
created. Let us look back at how <span class="docEmphasis">sbull</span> makes its
queue:</p>

<pre>dev-&gt;queue = blk_init_queue(sbull_request, &amp;dev-&gt;lock);</pre><br>


<p class="docText">Thus, when the queue
is<a name="chp-16-ITERM-7365"></a> created, the
<span class="docEmphasis">request</span> function is associated with it. We also
provided a spinlock as part of the queue creation process. Whenever
our <span class="docEmphasis">request</span> function is called, that lock is
held by the kernel. As a result, the <span class="docEmphasis">request</span>
function is running in an atomic context; it must follow all of the
usual rules for atomic code discussed in <a class="docLink" href="chp-5.shtml#chp-5">Chapter 5</a>.</p>

<p class="docText">The queue lock also prevents the kernel from queuing any other
requests for your device while your <span class="docEmphasis">request</span>
function holds the lock. Under some conditions, you may want to
consider dropping that lock while the <span class="docEmphasis">request</span>
function runs. If you do so, however, you must be sure not to access
the request queue, or any other data structure protected by the lock,
while the lock is not held. You must also reacquire the lock before
the <span class="docEmphasis">request</span> function returns.</P>

<p class="docText">Finally, the invocation of the <span class="docEmphasis">request</span> function
is (usually) entirely asynchronous with respect to the actions of any
user-space process. You cannot assume that the kernel is running in
the context of the process that initiated the current request. You do
not know if the I/O buffer provided by the request is in kernel or
user space. So any sort of operation that explicitly accesses user
space is in error and will certainly lead to trouble. As you will
see, everything your driver needs to know about the request is
contained within the structures passed to you via the request queue.</p>


<a name="chp-16-sect-3.2"></a>
<H4 class="docSection2Title">16.3.2. A Simple request Method</H4>

<p class="docText">The <span class="docEmphasis">sbull</span> example driver
<a name="chp-16-ITERM-7366"></a>
<a name="chp-16-ITERM-7367"></a>provides a few different methods for
request processing. By default, <span class="docEmphasis">sbull</span> uses a
method called <span class="docEmphasis">sbull_request</span>, which is meant to
be an example of the simplest possible <span class="docEmphasis">request</span>
method. Without further ado, here it is:</p>

<pre>static void sbull_request(request_queue_t *q)
{
    struct request *req;

    while ((req = elv_next_request(q)) != NULL) {
        struct sbull_dev *dev = req-&gt;rq_disk-&gt;private_data;
        if (! blk_fs_request(req)) {
            printk (KERN_NOTICE "Skip non-fs request\n");
            end_request(req, 0);
            continue;
        }
        sbull_transfer(dev, req-&gt;sector, req-&gt;current_nr_sectors,
                req-&gt;buffer, rq_data_dir(req));
        end_request(req, 1);
    }
}</pre><br>


<p class="docText">This function introduces the
<tt>struct</tt><a name="chp-16-ITERM-7368"></a>
<a name="chp-16-ITERM-7369"></a> <tt>request</tt> structure.
We will examine <tt>struct</tt> <tt>request</tt>
in great detail later on; for now, suffice it to say that it
represents a block I/O request for us to execute.</p>

<p class="docText">The kernel provides the function
<span class="docEmphasis">elv_next_request</span><a name="chp-16-ITERM-7370"></a>
<a name="chp-16-ITERM-7371"></a>
to obtain the first incomplete request on the queue; that function
returns <tt>NULL</tt> when there are no requests to be
processed. Note that <span class="docEmphasis">elv_next_request</span> does not
remove the request from the queue. If you call it twice with no
intervening operations, it returns the same
<tt>request</tt> structure both times. In this simple mode
of operation, requests are taken off the queue only when they are
complete.</P>

<p class="docText">A block request queue can contain requests that do not actually move
blocks to and from a disk. Such requests can include vendor-specific,
low-level diagnostics operations or instructions relating to
specialized device modes, such as the packet writing mode for
recordable media. Most block drivers do not know how to handle such
requests and simply fail them; <span class="docEmphasis">sbull</span> works in
this way as well. The call to <span class="docEmphasis">block_fs_request</span>
tells us whether we are looking at a filesystem request—one
that moves blocks of data. If a request is not a filesystem request,
we pass it to <span class="docEmphasis">end_request</span>:</p>

<pre>void end_request(struct request *req, int succeeded);</pre><BR>


<p class="docText">When we dispose of nonfilesystem requests, we pass
<tt>succeeded</tt> as <tt>0</tt> to indicate that
we did not successfully complete the request. Otherwise, we call
<span class="docEmphasis">sbull_transfer</span> to actually move the data, using
a set of fields provided in the <tt>request</tt> structure:</P>

<a name="chp-16-ITERM-7372"></a><a name="chp-16-ITERM-7373"></a><a name="chp-16-ITERM-7374"></a><a name="chp-16-ITERM-7375"></a><dl class="docList"><dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">sector_t sector;</span></span><a name="chp-16-ITERM-7372"></a></span></dt></p>
<dd>
<p class="docList">The index of the beginning sector on our device. Remember that this
sector number, like all such numbers passed between the kernel and
the driver, is expressed in 512-byte sectors. If your hardware uses a
different sector size, you need to scale <tt>sector</tt>
accordingly. For example, if the hardware uses 2048-byte sectors, you
need to divide the beginning sector number by four before putting it
into a request for the hardware.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned long nr_sectors;</span></span><a name="chp-16-ITERM-7373"></a></span></dt></P>
<dd>
<p class="docList">The number of (512-byte) sectors to be transferred.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">char *buffer;</span></span><a name="chp-16-ITERM-7374"></a></span></dt></P>
<dd>
<p class="docList">A pointer to the buffer to or from which the data should be
transferred. This pointer is a kernel virtual address and can be
dereferenced directly by the driver if need be.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">rq_data_dir(struct request *req);</span></span><a name="chp-16-ITERM-7375"></a></span></dt></p>
<dd>
<p class="docList">This macro extracts the direction of the transfer from the request; a
zero return value denotes a read from the device, and a nonzero
return value denotes a write to the device.</P>
</dd>
</dl>

<p class="docText">Given this information, the <span class="docEmphasis">sbull</span> driver can
implement the actual data transfer with a simple
<span class="docEmphasis">memcpy</span> call—our data is already in memory,
after all. The function that performs this copy operation
(<span class="docEmphasis">sbull_transfer</span>) also handles the scaling of
sector sizes and ensures that we do not try to copy beyond the end of
our virtual device:</p>

<pre>static void sbull_transfer(struct sbull_dev *dev, unsigned long sector,
        unsigned long nsect, char *buffer, int write)
{
    unsigned long offset = sector*KERNEL_sectOR_SIZE;
    unsigned long nbytes = nsect*KERNEL_sectOR_SIZE;

    if ((offset + nbytes) &gt; dev-&gt;size) {
        printk (KERN_NOTICE "Beyond-end write (%ld %ld)\n", offset, nbytes);
        return;
    }
    if (write)
        memcpy(dev-&gt;data + offset, buffer, nbytes);
    else
        memcpy(buffer, dev-&gt;data + offset, nbytes);
}</pre><br>


<p class="docText">With the code, <span class="docEmphasis">sbull</span> implements a complete,
simple RAM-based disk device. It is not, however, a realistic driver
for many types of devices, for a couple of reasons.</P>

<p class="docText">The first of those reasons is that <span class="docEmphasis">sbull</span>
executes requests synchronously, one at a time. High-performance disk
devices are capable of having numerous requests outstanding at the
same time; the disk's onboard controller can then
choose to execute them in the optimal order (one hopes). As long as
we process only the first request in the queue, we can never have
multiple requests being fulfilled at a given time. Being able to work
with more than one request requires a deeper understanding of request
queues and the <tt>request</tt> structure; the next few
sections help build that understanding.</p>

<p class="docText">There is another issue to consider, however. The best performance is
obtained from disk devices when the system performs large transfers
involving multiple sectors that are located together on the disk. The
highest cost in a disk operation is always the positioning of the
read and write heads; once that is done, the time required to
actually read or write the data is almost insignificant. The
developers who design and implement filesystems and virtual memory
subsystems understand this, so they do their best to locate related
data contiguously on the disk and to transfer as many sectors as
possible in a single request. The block subsystem also helps in this
regard; request queues contain a great deal of logic aimed at finding
adjacent requests and coalescing them into larger operations.</p>

<p class="docText">The <span class="docEmphasis">sbull</span> driver, however, takes all that work
and simply ignores it. Only one buffer is transferred at a time,
meaning that the largest single transfer is almost never going to
exceed the size of a single page. A block driver can do much better
than that, but it requires a deeper understanding of
<tt>request</tt> structures and the <tt>bio</tt>
structures from which requests are built.</P>

<p class="docText">The next few sections delve more deeply into how the block layer does
its job and the data structures that result from that work.</p>


<a name="chp-16-sect-3.3"></a>
<H4 class="docSection2Title">16.3.3. Request Queues</h4>

<p class="docText">In the simplest sense,
a<a name="chp-16-ITERM-7376"></a> block request queue is exactly that: a
queue of block I/O requests. If you look under the hood, a request
queue turns out to be a surprisingly complex data structure.
Fortunately, drivers need not worry about most of that complexity.</p>

<p class="docText">Request queues keep track of outstanding block I/O requests. But they
also play a crucial role in the creation of those requests. The
request queue stores parameters that describe what kinds of requests
the device is able to service: their maximum size, how many separate
segments may go into a request, the hardware sector size, alignment
requirements, etc. If your request queue is properly configured, it
should never present you with a request that your device cannot
handle.</p>

<p class="docText">Request queues also implement a plug-in interface that allows the use
of
multiple<a name="chp-16-ITERM-7377"></a>
<a name="chp-16-ITERM-7378"></a>
<a name="chp-16-ITERM-7379"></a>
<i>I/O schedulers</i> (or
<i>elevators</i>) to be used. An I/O
scheduler's job is to present I/O requests to your
driver in a way that maximizes performance. To this end, most I/O
schedulers accumulate a batch of requests, sort them into increasing
(or decreasing) block index order, and present the requests to the
driver in that order. The disk head, when given a sorted list of
requests, works its way from one end of the disk to the other, much
like a full elevator moves in a single direction until all of its
"requests" (people waiting to get
off) have been satisfied. The 2.6 kernel includes a
"<a name="chp-16-ITERM-7380"></a>deadline scheduler,"
which makes an effort to ensure that every request is satisfied
within a preset maximum time, and an "anticipatory
scheduler," which actually stalls a device briefly
after a read request in anticipation that another, adjacent read will
arrive almost immediately. As of this writing, the default scheduler
is the anticipatory scheduler, which seems to give the best
interactive system performance.</p>

<p class="docText">The I/O scheduler is also charged with merging adjacent requests.
When a new I/O request is handed to the scheduler, it searches the
queue for requests involving adjacent sectors; if one is found and if
the resulting request would not be too large, the two requests are
merged.</p>

<p class="docText">Request queues have a type of <tt>struct request_queue</tt>
or <tt>request_queue_t</tt>. This type, and the many
functions that operate on it, are defined in
<i>&lt;linux/blkdev.h&gt;</i>. If you are interested in
the implementation of request queues, you can find most of the code
in <i>drivers/block/ll_rw_block.c</i> and
<I>elevator.c</i>.</P>

<a name="chp-16-sect-3.3.1"></a>
<H5 class="docSection3Title">16.3.3.1 Queue creation and deletion</h5>

<p class="docText">As we saw in our
example<a name="chp-16-ITERM-7381"></a>
<a name="chp-16-ITERM-7382"></a>
<a name="chp-16-ITERM-7383"></a>
code, a request queue is a dynamic data structure that must be
created by the block I/O subsystem. The function to create and
initialize a request queue is:</p>

<pre>request_queue_t *blk_init_queue(request_fn_proc *request, spinlock_t *lock);</pre><br>


<p class="docText">The arguments are, of course, the <span class="docEmphasis">request</span>
function for this queue and a spinlock that controls access to the
queue. This function allocates memory (quite a bit of memory,
actually) and can fail because of this; you should always check the
return value before attempting to use the queue.</P>

<p class="docText">As part of the initialization of a request queue, you can set the
field <tt>queuedata</tt> (which is a <tt>void
*</tt> pointer) to any value you like. This field is the request
queue's equivalent to the
<tt>private_data</tt> we have seen in other structures.</p>

<p class="docText">To return a request queue to the system (at module unload time,
generally), call
<span class="docEmphasis">blk_cleanup_queue</span><a name="chp-16-ITERM-7384"></a>
<a name="chp-16-ITERM-7385"></a>:</P>

<pre>void blk_cleanup_queue(request_queue_t *);</pre><BR>


<p class="docText">After this call, your driver sees no more requests from the given
queue and should not reference it again.</P>



<a name="chp-16-sect-3.3.2"></a>
<h5 class="docSection3Title">16.3.3.2 Queueing functions</h5>

<p class="docText">There is a very small set
<a name="chp-16-ITERM-7386"></a>
<a name="chp-16-ITERM-7387"></a>of
functions for the manipulation of requests on queues—at least,
as far as drivers are concerned. You must hold the queue lock before
you call these functions.</p>

<p class="docText">The function that returns the next request to process is
<span class="docEmphasis">elv_next_request</span><a name="chp-16-ITERM-7388"></a>
<a name="chp-16-ITERM-7389"></a>:</p>

<pre>struct request *elv_next_request(request_queue_t *queue);</pre><BR>


<p class="docText">We have already seen this function in the simple
<span class="docEmphasis">sbull</span> example. It returns a pointer to the next
request to process (as determined by the I/O scheduler) or
<tt>NULL</tt> if no more requests remain to be processed.
<span class="docEmphasis">elv_next_request</span> leaves the request on the queue
but marks it as being active; this mark prevents the I/O scheduler
from attempting to merge other requests with this one once you start
to execute it.</P>

<p class="docText">To actually remove a request from a queue, use
<span class="docEmphasis">blkdev_dequeue_request</span><a name="chp-16-ITERM-7390"></a>
<a name="chp-16-ITERM-7391"></a>:</P>

<pre>void blkdev_dequeue_request(struct request *req);</pre><br>


<p class="docText">If your driver operates on multiple requests from the same queue
simultaneously, it must dequeue them in this manner.</P>

<p class="docText">Should you need to put a dequeued request back on the queue for some
reason, you can call:</P>

<pre>void elv_requeue_request(request_queue_t *queue, struct request *req);</pre><br>




<a name="chp-16-sect-3.3.3"></a>
<h5 class="docSection3Title">16.3.3.3 Queue control functions</h5>

<p class="docText">The block layer exports a<a name="chp-16-ITERM-7392"></a>
<a name="chp-16-ITERM-7393"></a> set of functions that can be used
by a driver to control how a request queue operates. These functions
include:</p>

<a name="chp-16-ITERM-7394"></a><a name="chp-16-ITERM-7395"></a><a name="chp-16-ITERM-7396"></a><a name="chp-16-ITERM-7397"></a><a name="chp-16-ITERM-7398"></a><a name="chp-16-ITERM-7399"></a><a name="chp-16-ITERM-7400"></a><a name="chp-16-ITERM-7401"></a><a name="chp-16-ITERM-7402"></a><a name="chp-16-ITERM-7403"></a><a name="chp-16-ITERM-7404"></a><a name="chp-16-ITERM-7405"></a><a name="chp-16-ITERM-7406"></a><a name="chp-16-ITERM-7407"></a><a name="chp-16-ITERM-7408"></a><a name="chp-16-ITERM-7409"></a><a name="chp-16-ITERM-7410"></a><a name="chp-16-ITERM-7411"></a><a name="chp-16-ITERM-7412"></a><a name="chp-16-ITERM-7413"></a><a name="chp-16-ITERM-7414"></a><a name="chp-16-ITERM-7415"></a><a name="chp-16-ITERM-7416"></a><a name="chp-16-ITERM-7417"></a><dl class="docList"><dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_stop_queue(request_queue_t *queue);</span></span><a name="chp-16-ITERM-7394"></a>
<a name="chp-16-ITERM-7395"></a></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_start_queue(request_queue_t *queue);</span></span><a name="chp-16-ITERM-7396"></a>
<a name="chp-16-ITERM-7397"></a></span></dt></p>
<dd>
<p class="docList">If your device has reached a state where it can handle no more
outstanding commands, you can call
<span class="docEmphasis">blk_stop_queue</span> to tell the block layer. After
this call, your <span class="docEmphasis">request</span> function will not be
called until you call <span class="docEmphasis">blk_start_queue</span>. Needless
to say, you should not forget to restart the queue when your device
can handle more requests. The queue lock must be held when calling
either of these functions.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_bounce_limit(request_queue_t *queue, u64 dma_addr);</span></span><a name="chp-16-ITERM-7398"></a>
<a name="chp-16-ITERM-7399"></a></span></dt></p>
<dd>
<p class="docList">Function that tells the kernel the highest physical address to which
your device can perform DMA. If a request comes in containing a
reference to memory above the limit, a bounce buffer will be used for
the operation; this is, of course, an expensive way to perform block
I/O and should be avoided whenever possible. You can provide any
reasonable physical address in this argument, or make use of the
predefined symbols
<tt>BLK_BOUNCE_HIGH</tt><a name="chp-16-ITERM-7400"></a>
<a name="chp-16-ITERM-7401"></a>
(use <a name="chp-16-ITERM-7402"></a>
<a name="chp-16-ITERM-7403"></a>bounce buffers for high-memory pages),
<tt>BLK_BOUNCE_ISA</tt> (the driver can DMA only into the
16-MB ISA zone), or <tt>BLK_BOUNCE_ANY</tt> (the driver can
perform DMA to any address). The default value is
<tt>BLK_BOUNCE_HIGH</tt>.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_max_sectors(request_queue_t *queue, unsigned short max);</span></span><a name="chp-16-ITERM-7404"></a>
<a name="chp-16-ITERM-7405"></a></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_max_phys_segments(request_queue_t *queue, unsigned short max);</span></span><a name="chp-16-ITERM-7406"></a>
<a name="chp-16-ITERM-7407"></a></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_max_hw_segments(request_queue_t *queue, unsigned short max);</span></span><a name="chp-16-ITERM-7408"></a>
<a name="chp-16-ITERM-7409"></a></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_max_segment_size(request_queue_t *queue, unsigned int max);</span></span><a name="chp-16-ITERM-7410"></a>
<a name="chp-16-ITERM-7411"></a></span></dt></P>
<dd>
<p class="docList">Functions that set parameters describing the requests that can be
satisfied by this device. <span class="docEmphasis">blk_queue_max_sectors</span>
can be used to set the maximum size of any request in (512-byte)
sectors; the default is 255.
<span class="docEmphasis">blk_queue_max_phys_segments</span> and
<span class="docEmphasis">blk_queue_max_hw_segments</span> both control how many
physical segments (nonadjacent areas in system memory) may be
contained within a single request. Use
<span class="docEmphasis">blk_queue_max_phys_segments</span> to say how many
segments your driver is prepared to cope with; this may be the size
of a staticly allocated scatterlist, for example.
<span class="docEmphasis">blk_queue_max_hw_segments</span>, in contrast, is the
maximum number of segments that the device itself can handle. Both of
these parameters default to 128. Finally,
<span class="docEmphasis">blk_queue_max_segment_size</span> tells the kernel how
large any individual segment of a request can be in bytes; the
default is 65,536 bytes.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">blk_queue_segment_boundary(request_queue_t *queue, unsigned long mask);</span></span><a name="chp-16-ITERM-7412"></a>
<a name="chp-16-ITERM-7413"></a></span></dt></p>
<dd>
<p class="docList">Some devices cannot handle requests that cross a particular size
memory boundary; if your device is one of those, use this function to
tell the kernel about that boundary. For example, if your device has
trouble with requests that cross a 4-MB boundary, pass in a mask of
<tt>0x3fffff</tt>. The default mask is
<tt>0xffffffff</tt>.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_dma_alignment(request_queue_t *queue, int mask);</span></span><a name="chp-16-ITERM-7414"></a>
<a name="chp-16-ITERM-7415"></a></span></dt></p>
<dd>
<p class="docList">Function that tells the kernel about the memory alignment constraints
your device imposes on DMA transfers. All requests are created with
the given alignment, and the length of the request also matches the
alignment. The default mask is <tt>0x1ff</tt>, which causes
all requests to be aligned on 512-byte boundaries.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void blk_queue_hardsect_size(request_queue_t *queue, unsigned short max);</span></span><a name="chp-16-ITERM-7416"></a>
<a name="chp-16-ITERM-7417"></a></span></dt></p>
<dd>
<p class="docList">Tells the kernel about your device's hardware sector
size. All requests generated by the kernel are a multiple of this
size and are properly aligned. All communications between the block
layer and the driver continues to be expressed in 512-byte sectors,
however.</p>
</dd>
</dl>



<a name="chp-16-sect-3.4"></a>
<h4 class="docSection2Title">16.3.4. The Anatomy of a Request</h4>

<p class="docText">In our simple example, we encountered the <tt>request</tt>
structure. However, we have barely scratched the surface of that
complicated data structure. In this section, we look, in some detail,
at how block I/O requests are represented in the Linux kernel.</P>

<p class="docText">Each <tt>request</tt> structure represents one block I/O
request, although it may have been formed through a merger of several
independent requests at a higher level. The sectors to be transferred
for any particular request may be distributed throughout main memory,
although they always correspond to a set of consecutive sectors on
the block device. The request is represented as a set of segments,
each of which corresponds to one in-memory buffer. The kernel may
join multiple requests that involve adjacent sectors on the disk, but
it never combines read and write operations within a single
<tt>request</tt> structure. The kernel also makes sure not
to combine requests if the result would violate any of the request
queue limits described in the previous section.</P>

<p class="docText">A <tt>request</tt> structure is implemented, essentially,
as a linked list of <tt>bio</tt> structures combined with
some housekeeping information to enable the driver to keep track of
its position as it works through the request. The
<tt>bio</tt> structure is a low-level description of a
portion of a block I/O request; we take a look at it now.</P>

<a name="chp-16-sect-3.4.1"></a>
<h5 class="docSection3Title">16.3.4.1 The bio structure</H5>

<p class="docText">When the kernel, in the form of a filesystem, the virtual memory
subsystem, or a system call, decides that a set of blocks must be
transferred to or from a block I/O device; it puts together a
<tt>bio</tt> structure to describe that operation. That
structure is then handed to the block I/O code, which merges it into
an existing <tt>request</tt> structure or, if need be,
creates a new one. The <tt>bio</tt> structure contains
everything that a block driver needs to carry out the request without
reference to the user-space process that caused that request to be
initiated.</P>

<p class="docText">The <tt>bio</tt><a name="chp-16-ITERM-7418"></a>
<a name="chp-16-ITERM-7419"></a>
structure, which is defined in
<i>&lt;linux/bio.h&gt;</i>, contains a number of fields
that may be of use to driver authors:</p>

<a name="chp-16-ITERM-7420"></a><a name="chp-16-ITERM-7421"></a><a name="chp-16-ITERM-7422"></a><a name="chp-16-ITERM-7423"></a><a name="chp-16-ITERM-7424"></a><dl class="docList"><dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">sector_t bi_sector;</span></span><a name="chp-16-ITERM-7420"></a></span></dt></p>
<dd>
<p class="docList">The first (512-byte) sector to be transferred for this
<tt>bio</tt>.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned int bi_size;</span></span><a name="chp-16-ITERM-7421"></a></span></dt></p>
<dd>
<p class="docList">The size of the data to be transferred, in bytes. Instead, it is
often easier to use <tt>bio_sectors(bio)</tt>, a macro that
gives the size in sectors.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned long bi_flags;</span></span><a name="chp-16-ITERM-7422"></a></span></dt></p>
<dd>
<p class="docList">A set of flags describing the <tt>bio</tt>; the least
significant bit is set if this is a write request (although the macro
<tt>bio_data_dir(bio)</tt> should be used instead of
looking at the flags directly).</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned short bio_phys_segments;</span></span><a name="chp-16-ITERM-7423"></a></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned short bio_hw_segments;</span></span><a name="chp-16-ITERM-7424"></a></span></dt></p>
<dd>
<p class="docList">The number of physical segments contained within this BIO and the
number of segments seen by the hardware after DMA mapping is done,
respectively.</p>
</dd>
</dl>

<p class="docText">The core of a <tt>bio</tt>, however, is an array called
<tt>bi_io_vec</tt><a name="chp-16-ITERM-7425"></a>
<a name="chp-16-ITERM-7426"></a>,
which is made up of the following structure:</p>

<pre>struct bio_vec {
        struct page     *bv_page;
        unsigned int    bv_len;
        unsigned int    bv_offset;
};</pre><br>


<p class="docText"><a class="docLink" href="chp-16-sect-3.shtml#chp-16-FIG-1">Figure 16-1</a> shows how these
structures all tie together. As you can see, by the time a block I/O
request is turned into a <tt>bio</tt> structure, it has
been broken down into individual pages of physical memory. All a
driver needs to do is to step through this array of structures (there
are <tt>bi_vcnt</tt> of them), and transfer data within
each page (but only <tt>len</tt> bytes starting at
<tt>offset</tt>).</p>

<a name="chp-16-FIG-1"></a><p><center>
<H5 class="docFigureTitle">Figure 16-1. The bio structure</h5>
<img border="0" alt=""  width="267" height="160" SRC="images/0596005903/figs/ldr3_1601.gif"></center></P><BR>

<p class="docText">Working directly with the <tt>bi_io_vec</tt> array is
discouraged in the interest of kernel developers being able to change
the <tt>bio</tt> structure in the future without breaking
things. To that end, a set of macros has been provided to ease the
process of working with the <tt>bio</tt> structure. The
place to start is with <tt>bio_for_each_segment</tt>, which
simply loops through every unprocessed entry in the
<tt>bi_io_vec</tt> array. This macro should be used as
follows:</p>

<pre>int segno;
struct bio_vec *bvec;

bio_for_each_segment(bvec, bio, segno) {
    /* Do something with this segment
}</pre><br>


<p class="docText">Within this loop, <tt>bvec</tt> points to the current
<tt>bio_vec</tt> entry, and <tt>segno</tt> is the
current segment number. These values can be used to set up DMA
transfers (an alternative way using
<span class="docEmphasis">blk_rq_map_sg</span> is described in <a class="docLink" href="chp-16-sect-3.shtml#chp-16-sect-3.5.2">Section 16.3.5.2</a>). If you need to access the pages directly,
you should first ensure that a proper kernel virtual address exists;
to that end, you can use:</P>

<pre>char *_ _bio_kmap_atomic(struct bio *bio, int i, enum km_type type);
void _ _bio_kunmap_atomic(char *buffer, enum km_type type);</pre><br>


<p class="docText">This low-level function allows you to directly map the buffer found
in a given <tt>bio_vec</tt>, as indicated by the index
<tt>i</tt>. An atomic kmap is created; the caller must
provide the appropriate slot to use (as described in the section
<a class="docLink" href="chp-15-sect-1.shtml#chp-15-sect-1.4">Section 15.1.4</a>).</P>

<p class="docText">The block layer also maintains a set of pointers within the
<tt>bio</tt> structure to keep track of the current
<a name="chp-16-ITERM-7427"></a>
<a name="chp-16-ITERM-7428"></a>state of request processing.
Several macros exist to provide access to that state:</P>

<dl class="docList"><dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">struct page *bio_page(struct bio *bio);</span></span></span></dt></p>
<dd>
<p class="docList">Returns a pointer to the <tt>page</tt> structure
representing the page to be transferred next.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">int bio_offset(struct bio *bio);</span></span></span></dt></P>
<dd>
<p class="docList">Returns the offset within the page for the data to be transferred.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">int bio_cur_sectors(struct bio *bio);</span></span></span></dt></P>
<dd>
<p class="docList">Returns the number of sectors to be transferred out of the current
page.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">char *bio_data(struct bio *bio);</span></span></span></dt></p>
<dd>
<p class="docList">Returns a kernel logical address pointing to the data to be
transferred. Note that this address is available only if the page in
question is not located in high memory; calling it in other
situations is a bug. By default, the block subsystem does not pass
high-memory buffers to your driver, but if you have changed that
setting with <span class="docEmphasis">blk_queue_bounce_limit</span>, you
probably should not be using <tt>bio_data</tt>.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">char *bio_kmap_irq(struct bio *bio, unsigned long *flags);</span></span></span></dt></P>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">void bio_kunmap_irq(char *buffer, unsigned long *flags);</span></span></span></dt></P>
<dd>
<p class="docList"><span class="docEmphasis">bio_kmap_irq</span> returns a kernel virtual address
for any buffer, regardless of whether it resides in high or low
memory. An atomic kmap is used, so your driver cannot sleep while
this mapping is active. Use <span class="docEmphasis">bio_kunmap_irq</span> to
unmap the buffer. Note that the <tt>flags</tt> argument is
passed by pointer here. Note also that since an atomic kmap is used,
you cannot map more than one segment at a time.</p>
</dd>
</dl>

<p class="docText">All of the functions just described access the
"current" buffer—the first
buffer that, as far as the kernel knows, has not been transferred.
Drivers often want to work through several buffers in the
<tt>bio</tt> before signaling completion on any of them
(with <span class="docEmphasis">end_that_request_first</span>, to be described
shortly), so these functions are often not useful. Several other
macros exist for working with the internals of the
<tt>bio</tt> structure (see
<I>&lt;linux/bio.h&gt;</i> for details).</p>



<a name="chp-16-sect-3.4.2"></a>
<h5 class="docSection3Title">16.3.4.2 Request structure fields</h5>

<p class="docText">Now that we have an idea of how the <tt>bio</tt> structure
works, we can get deep into <tt>struct</tt>
<tt>request</tt> and see how request processing works. The
fields of this structure include:</p>

<dl class="docList"><dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">sector_t hard_sector;</span></span></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned long hard_nr_sectors;</span></span></span></dt></p>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned int hard_cur_sectors;</span></span></span></dt></P>
<dd>
<p class="docList">Fields that track the sectors that the driver has yet to complete.
The first sector that has <span class="docEmphasis">not</span> been transferred
is stored in <tt>hard_sector</tt>, the total number of
sectors yet to transfer is in <tt>hard_nr_sectors</tt>, and
the number of sectors remaining in the current <tt>bio</tt>
is <tt>hard_cur_sectors</tt>. These fields are intended for
use only within the block subsystem; drivers should not make use of
them.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">struct bio *bio;</span></span></span></dt></p>
<dd>
<p class="docList"><tt>bio</tt> is the linked list of <tt>bio</tt>
structures for this request. You should not access this field
directly; use <span class="docEmphasis">rq_for_each_bio</span> (described later)
instead.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">char *buffer;</span></span></span></dt></p>
<dd>
<p class="docList">The simple driver example earlier in this chapter used this field to
find the buffer for the transfer. With our deeper understanding, we
can now see that this field is simply the result of calling
<span class="docEmphasis">bio_data</span> on the current <tt>bio</tt>.</P>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">unsigned short nr_phys_segments;</span></span></span></dt></p>
<dd>
<p class="docList">The number of distinct segments occupied by this request in physical
memory after adjacent pages have been merged.</p>
</dd>
<dt><span class="docPubcolor"><span class="docPubcolor"><span class="docMonofont">struct list_head queuelist;</span></span></span></dt></P>
<dd>
<p class="docList">The linked-list structure (as described in <a class="docLink" href="chp-11-sect-5.shtml#chp-11-sect-5">Section 11.5</a>)
that links the request
into the request queue. If (and only if) you remove the request from
the queue with <span class="docEmphasis">blkdev_dequeue_request</span>, you may
use this list head to track the request in an internal list
maintained by your driver.</P>
</dd>
</dl>

<p class="docText"><a class="docLink" href="chp-16-sect-3.shtml#chp-16-FIG-2">Figure 16-2</a> shows how the
request structure and its component bio structures fit together. In
the figure, the request has been partially satisfied; the
<tt>cbio</tt> and <tt>buffer</tt> fields point to
the first bio that has not yet been transferred.</P>

<a name="chp-16-FIG-2"></a><p><center>
<H5 class="docFigureTitle">Figure 16-2. A request queue with a partially processed request</H5>
<img border="0" alt=""  width="264" height="200" SRC="images/0596005903/figs/ldr3_1602.gif"></center></p><br>

<p class="docText">There are many other fields inside the <tt>request</tt>
structure, but the list in this section should be enough for most
driver writers.</p>



<a name="chp-16-sect-3.4.3"></a>
<h5 class="docSection3Title">16.3.4.3 Barrier requests</H5>

<p class="docText">The block layer reorders
requests<a name="chp-16-ITERM-7429"></a>
before your driver sees them to improve I/O performance. Your driver,
too, can reorder requests if there is a reason to do so. Often, this
reordering happens by passing multiple requests to the drive and
letting the hardware figure out the optimal ordering. There is a
problem with unrestricted reordering of requests, however: some
applications require guarantees that certain operations will complete
before others are started. Relational database managers, for example,
must be absolutely sure that their journaling information has been
flushed to the drive before executing a transaction on the database
contents. Journaling filesystems, which are now in use on most Linux
systems, have very similar ordering constraints. If the wrong
operations are reordered, the result can be severe, undetected data
corruption.</p>

<p class="docText">The 2.6 block layer addresses this problem with the concept of a
<i>barrier request</I>. If a request is marked with
the <tt>REQ_HARDBARRER</tt> flag, it must be written to the
drive before any following request is initiated. By
"written to the drive," we mean
that the data must actually reside and be persistent on the physical
media. Many drives perform caching of write requests; this caching
improves performance, but it can defeat the purpose of barrier
requests. If a power failure occurs when the critical data is still
sitting in the drive's cache, that data is still
lost even if the drive has reported completion. So a driver that
implements barrier requests must take steps to force the drive to
actually write the data to the media.</p>

<p class="docText">If your driver honors barrier requests, the first step is to inform
the block layer of this fact. Barrier handling is another of the
request queues; it is set with:</p>

<pre>void blk_queue_ordered(request_queue_t *queue, int flag);</pre><BR>


<p class="docText">To indicate that your driver implements barrier requests, set the
<tt>flag</tt> parameter to a nonzero value.</p>

<p class="docText">The actual implementation of barrier requests is simply a matter of
testing for the associated flag in the <tt>request</tt>
structure. A macro has been provided to perform this test:</P>

<pre>int blk_barrier_rq(struct request *req);</pre><br>


<p class="docText">If this macro returns a nonzero value, the request is a barrier
request. Depending on how your hardware works, you may have to stop
taking requests from the queue until the barrier request has been
completed. Other drives can understand barrier requests themselves;
in this case, all your driver has to do is to issue the proper
operations for those drives.</p>



<a name="chp-16-sect-3.4.4"></a>
<h5 class="docSection3Title">16.3.4.4 Nonretryable requests</h5>

<p class="docText">Block drivers often<a name="chp-16-ITERM-7430"></a> attempt to retry requests that fail
the first time. This behavior can lead to a more reliable system and
help to avoid data loss. The kernel, however, sometimes marks
requests as not being retryable. Such requests should simply fail as
quickly as possible if they cannot be executed on the first try.</p>

<p class="docText">If your driver is considering retrying a failed request, it should
first make a call to:</p>

<pre>int blk_noretry_request(struct request *req);</pre><br>


<p class="docText">If this macro returns a nonzero value, your driver should simply
abort the request with an error code instead of retrying it.</p>



<a name="chp-16-sect-3.5"></a>
<h4 class="docSection2Title">16.3.5. Request Completion Functions</h4>

<p class="docText">There are, as we will see,
<a name="chp-16-ITERM-7431"></a>several different ways of working
through a <tt>request</tt> structure. All of them make use
of a couple of common functions, however, which handle the completion
of an I/O request or parts of a request. Both of these functions are
atomic and can be safely called from an atomic context.</p>

<p class="docText">When your device has completed transferring some or all of the
sectors in an I/O request, it must inform the block subsystem with:</p>

<pre>int end_that_request_first(struct request *req, int success, int count);</pre><br>


<p class="docText">This function tells the block code that your driver has finished with
the transfer of <tt>count</tt> sectors starting where you
last left off. If the I/O was successful, pass
<tt>success</tt> as <tt>1</tt>; otherwise pass
<tt>0</tt>. Note that you must signal completion in order
from the first sector to the last; if your driver and device somehow
conspire to complete requests out of order, you have to store the
out-of-order completion status until the intervening sectors have
been transferred.</P>

<p class="docText">The return value from <span class="docEmphasis">end_that_request_first</span> is
an indication of whether all sectors in this request have been
transferred or not. A return value of <tt>0</tt> means that
all sectors have been transferred and that the request is complete.
At that point, you must dequeue the request with
<span class="docEmphasis">blkdev_dequeue_request</span> (if you have not already
done so) and pass it to:</p>

<pre>void end_that_request_last(struct request *req);</pre><BR>


<p class="docText"><span class="docEmphasis">end_that_request_last</span> informs whoever is waiting
for the request that it has completed and recycles the
<tt>request</tt> structure; it must be called with the
queue lock held.</P>

<p class="docText">In our simple <span class="docEmphasis">sbull</span> example, we
didn't use any of the above functions. That example,
instead, is called <span class="docEmphasis">end_request</span>. To show the
effects of this call, here is the entire
<span class="docEmphasis">end_request</span> function as seen in the 2.6.10
kernel:</p>

<pre>void end_request(struct request *req, int uptodate)
{
    if (!end_that_request_first(req, uptodate, req-&gt;hard_cur_sectors)) {
        add_disk_randomness(req-&gt;rq_disk);
        blkdev_dequeue_request(req);
        end_that_request_last(req);
    }
}</pre><br>


<p class="docText">The function <span class="docEmphasis">add_disk_randomness</span> uses the timing
of block I/O requests to contribute entropy to the
system's random number pool; it should be called
only if the disk's timing is truly random. That is
true for most mechanical devices, but it is not true for a
memory-based virtual device, such as <span class="docEmphasis">sbull</span>. For
this reason, the more complicated version of
<span class="docEmphasis">sbull</span> shown in the next section does not call
<span class="docEmphasis">add_disk_randomness</span>.</p>

<a name="chp-16-sect-3.5.1"></a>
<H5 class="docSection3Title">16.3.5.1 Working with bios</h5>

<p class="docText">You now know enough to <a name="chp-16-ITERM-7432"></a>
<a name="chp-16-ITERM-7433"></a>write
a block driver that works directly with the <tt>bio</tt>
structures that make up a request. An example might help, however. If
the <span class="docEmphasis">sbull</span> driver is loaded with the
<tt>request_mode</tt> parameter set to
<tt>1</tt>, it registers a <tt>bio</tt>-aware
<span class="docEmphasis">request</span> function instead of the simple function
we saw above. That function looks like this:</P>

<pre>static void sbull_full_request(request_queue_t *q)
{
    struct request *req;
    int sectors_xferred;
    struct sbull_dev *dev = q-&gt;queuedata;

    while ((req = elv_next_request(q)) != NULL) {
        if (! blk_fs_request(req)) {
            printk (KERN_NOTICE "Skip non-fs request\n");
            end_request(req, 0);
            continue;
        }
        sectors_xferred = sbull_xfer_request(dev, req);
        if (! end_that_request_first(req, 1, sectors_xferred)) {
            blkdev_dequeue_request(req);
            end_that_request_last(req);
        }
    }
}</pre><BR>


<p class="docText">This function simply takes each request, passes it to
<span class="docEmphasis">sbull_xfer_request</span>, then completes it with
<span class="docEmphasis">end_that_request_first</span> and, if necessary,
<span class="docEmphasis">end_that_request_last</span>. Thus, this function is
handling the high-level queue and request management parts of the
problem. The job of actually executing a request, however, falls to
<span class="docEmphasis">sbull_xfer_request</span>:</P>

<pre>static int sbull_xfer_request(struct sbull_dev *dev, struct request *req)
{
    struct bio *bio;
    int nsect = 0;
    
    rq_for_each_bio(bio, req) {
        sbull_xfer_bio(dev, bio);
        nsect += bio-&gt;bi_size/KERNEL_sectOR_SIZE;
    }
    return nsect;
}</pre><br>


<p class="docText">Here we introduce another macro:
<span class="docEmphasis">rq_for_each_bio</span>. As you might expect, this macro
simply steps through each <tt>bio</tt> structure in the
request, giving us a pointer that we can pass to
<span class="docEmphasis">sbull_xfer_bio</span> for the transfer. That function
looks like:</p>

<pre>static int sbull_xfer_bio(struct sbull_dev *dev, struct bio *bio)
{
    int i;
    struct bio_vec *bvec;
    sector_t sector = bio-&gt;bi_sector;

    /* Do each segment independently. */
    bio_for_each_segment(bvec, bio, i) {
        char *buffer = _ _bio_kmap_atomic(bio, i, KM_USER0);
        sbull_transfer(dev, sector, bio_cur_sectors(bio),
                buffer, bio_data_dir(bio) =  = WRITE);
        sector += bio_cur_sectors(bio);
        _ _bio_kunmap_atomic(bio, KM_USER0);
    }
    return 0; /* Always "succeed" */
}</pre><br>


<p class="docText">This function simply steps through each segment in the
<tt>bio</tt> structure, gets a kernel virtual address to
access the buffer, then calls the same
<span class="docEmphasis">sbull_transfer</span> function we saw earlier to copy
the data over.</p>

<p class="docText">Each device has its own needs, but, as a general rule, the code just
shown should serve as a model for many situations where digging
through the <tt>bio</tt> structures is needed.</P>



<a name="chp-16-sect-3.5.2"></a>
<H5 class="docSection3Title">16.3.5.2 Block requests and DMA</H5>

<p class="docText">If you are working on a <a name="chp-16-ITERM-7434"></a>high-performance block driver, chances are
you will be using DMA for the actual data transfers. A block driver
can certainly step through the <tt>bio</tt> structures, as
described above, create a DMA mapping for each one, and pass the
result to the device. There is an easier way, however, if your device
can do scatter/gather I/O. The function:</p>

<pre>int blk_rq_map_sg(request_queue_t *queue, struct request *req, 
                  struct scatterlist *list);</pre><BR>


<p class="docText">fills in the given <tt>list</tt> with the full set of
segments from the given request. Segments that are adjacent in memory
are coalesced prior to insertion into the scatterlist, so you need
not try to detect them yourself. The return value is the number of
entries in the list. The function also passes back, in its third
argument, a scatterlist suitable for passing to
<span class="docEmphasis">dma_map_sg</span>. (See <a class="docLink" href="chp-15-sect-4.shtml#chp-15-sect-4.4.7">Section 15.4.4.7</a> 
for more information on
<span class="docEmphasis">dma_map_sg</span>.)</P>

<p class="docText">Your driver must allocate the storage for the scatterlist before
calling <span class="docEmphasis">blk_rq_map_sg</span>. The list must be able to
hold at least as many entries as the request has physical segments;
the <tt>struct</tt> <tt>request</tt> field
<tt>nr_phys_segments</tt> holds that count, which will not
exceed the maximum number of physical segments specified with
<span class="docEmphasis">blk_queue_max_phys_segments</span>.</p>

<p class="docText">If you do not want <span class="docEmphasis">blk_rq_map_sg</span> to coalesce
adjacent segments, you can change the default behavior with a call
such as:</p>

<pre>clear_bit(QUEUE_FLAG_CLUSTER, &amp;queue-&gt;queue_flags);</pre><br>


<p class="docText">Some SCSI disk drivers mark their request queue in this way, since
they do not benefit from the coalescing of requests.</p>



<a name="chp-16-sect-3.5.3"></a>
<H5 class="docSection3Title">16.3.5.3 Doing without a request queue</h5>

<p class="docText">Previously, we have discussed the work the kernel does to optimize
the order of requests in the queue; this work involves sorting
requests and, perhaps, even stalling the queue to allow an
anticipated request to arrive. These techniques help the
system's performance when dealing with a real,
spinning disk drive. They are completely wasted, however, with a
device like <span class="docEmphasis">sbull</span>. Many block-oriented devices,
such as flash memory arrays, readers for media cards used in digital
cameras, and RAM disks have truly random-access performance and do
not benefit from advanced-request queueing logic. Other devices, such
as software RAID arrays or virtual disks created by logical volume
managers, do not have the performance characteristics for which the
block layer's request queues are optimized. For this
kind of device, it would be better to accept requests directly from
the block layer and not bother with the request queue at all.</p>

<p class="docText">For these situations, the block layer supports a "no
queue" mode of operation. To make use of this mode,
your driver must provide a "make
request" function, rather than a
<span class="docEmphasis">request</span> function. The
<span class="docEmphasis">make_request</span> function has this prototype:</P>

<pre>typedef int (make_request_fn) (request_queue_t *q, struct bio *bio);</pre><br>


<p class="docText">Note that a request queue is still present, even though it will never
actually hold any requests. The <span class="docEmphasis">make_request</span>
function takes as its main parameter a <tt>bio</tt>
structure, which represents one or more buffers to be transferred.
The <span class="docEmphasis">make_request</span> function can do one of two
things: it can either perform the transfer directly, or it can
redirect the request to another device.</p>

<p class="docText">Performing the transfer directly is just a matter of working through
the <tt>bio</tt> with the accessor methods we described
earlier. Since there is no <tt>request</tt> structure to
work with, however, your function should signal completion directly
to the creator of the <tt>bio</tt> structure with a call to
<span class="docEmphasis">bio_endio</span>:</P>

<pre>void bio_endio(struct bio *bio, unsigned int bytes, int error);</pre><br>


<p class="docText">Here, <tt>bytes</tt> is the number of bytes you have
transferred so far. It can be less than the number of bytes
represented by the <tt>bio</tt> as a whole; in this way,
you can signal partial completion, and update the internal
"current buffer" pointers within
the <tt>bio</tt>. You should either call
<span class="docEmphasis">bio_endio</span> again as your device makes further
process, or signal an error if you are unable to complete the
request. Errors are indicated by providing a nonzero value for the
<tt>error</tt> parameter; this value is normally an error
code such as <tt>-EIO</tt>. The
<span class="docEmphasis">make_request</span> should return <tt>0</tt>,
regardless of whether the I/O is successful.</P>

<p class="docText">If <span class="docEmphasis">sbull</span> is loaded with
<tt>request_mode=2</tt>, it operates with a
<span class="docEmphasis">make_request</span> function. Since
<span class="docEmphasis">sbull</span> already has a function that can transfer a
single <tt>bio</tt>, the <span class="docEmphasis">make_request</span>
function is simple:</p>

<pre>static int sbull_make_request(request_queue_t *q, struct bio *bio)
{
    struct sbull_dev *dev = q-&gt;queuedata;
    int status;

    status = sbull_xfer_bio(dev, bio);
    bio_endio(bio, bio-&gt;bi_size, status);
    return 0;
}</pre><br>


<p class="docText">Please note that you should never call <span class="docEmphasis">bio_endio</span>
from a regular <span class="docEmphasis">request</span> function; that job is
handled by <span class="docEmphasis">end_that_request_first</span> instead.</p>

<p class="docText">Some block drivers, such as those implementing volume managers and
software RAID arrays, really need to redirect the request to another
device that handles the actual I/O. Writing such a driver is beyond
the scope of this book. We note, however, that if the
<span class="docEmphasis">make_request</span> function returns a nonzero value,
the <tt>bio</tt> is submitted again. A
"stacking" driver can, therefore,
modify the <tt>bi_bdev</tt> field to point to a different
device, change the starting sector value, then return; the block
system then passes the <tt>bio</tt> to the new device.
There is also a <span class="docEmphasis">bio_split</span> call that can be used
to split a <tt>bio</tt> into multiple chunks for submission
to more than one device. Although if the queue parameters are set up
correctly, splitting a <tt>bio</tt> in this way should
almost never be necessary.</p>

<p class="docText">Either way, you must tell the block subsystem that your driver is
using a custom <span class="docEmphasis">make_request</span> function. To do so,
you must allocate a request queue with:</p>

<pre>request_queue_t *blk_alloc_queue(int flags);</pre><br>


<p class="docText">This function differs from <span class="docEmphasis">blk_init_queue</span> in
that it does not actually set up the queue to hold requests. The
<tt>flags</tt> argument is a set of allocation flags to be
used in allocating memory for the queue; usually the right value is
<tt>GFP_KERNEL</tt>. Once you have a queue, pass it and
your <span class="docEmphasis">make_request</span> function to
<span class="docEmphasis">blk_queue_make_request</span>:</p>

<pre>void blk_queue_make_request(request_queue_t *queue, make_request_fn *func);</pre><br>


<p class="docText">The <span class="docEmphasis">sbull</span> code to set up the
<span class="docEmphasis">make_request</span> function looks like:</p>

<pre>dev-&gt;queue = blk_alloc_queue(GFP_KERNEL);
if (dev-&gt;queue =  = NULL)
    goto out_vfree;
blk_queue_make_request(dev-&gt;queue, sbull_make_request);</pre><br>


<p class="docText">For the curious, some time spent digging through
<i>drivers/block/ll_rw_block.c</i> shows that all
queues have a <span class="docEmphasis">make_request</span> function. The default
version, <span class="docEmphasis">generic_make_request</span>, handles the
incorporation of the <tt>bio</tt> into a
<tt>request</tt> structure. By providing a
<span class="docEmphasis">make_request</span> function of its own, a driver is
really just overriding a specific <span class="docEmphasis">request queue</span>
method and<a name="chp-16-ITERM-7435"></a> <a name="chp-16-ITERM-7436"></a> <a name="chp-16-ITERM-7437"></a> <a name="chp-16-ITERM-7438"></a> sorting out much of the work.</p>




<UL></ul></TD></TR></table>
<table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#e6e6e6">
<tr >
<td class="v2" align="left" width="30%">
<a href="chp-16-sect-2.shtml"> &#8678; prev </a>
</td>
<td class="v2" align="center" width="40%">
<a href="index.html" target=_parent style="text-decoration:none;text-underline:none"> &#8689; home </a>
</td>
<td class="v2" align="right" width="30%">
<a href="chp-16-sect-4.shtml"> next &#8680; </a>
</td>
</tr>
</table>
<script type="text/javascript" src="http://j.maxmind.com/app/geoip.js"></script>
<center>
<table width=100% height=90>
<tr style='display:none'>
<td style='display:xnone' id='banner' xalign=center style="background-image:url(/kernel_map.d/LKM3_2048.png);width:100%;height:90;opacity:0;filter:alpha(opacity=0);
cursor:pointer" onclick="top.location='http://www.makelinux.com/kernel_map?b'" /> 
</td> </tr>
<tr style='display:none'>
<td  id='banner2' style="opacity:0;filter:alpha(opacity=0);text-align:center;" /> 
<a target=_top href=http://www.makelinux.com/kernel_map_poster?b>
<span style="font-weight:bold"><span style="display:block;font-size:large" >Poster of Linux kernel</span>The best gift for a Linux geek</span>
</a>
</td> </tr>
<tr style='zdisplay:xnone' >
<td  id='banner3' align=center /> 
<a Xtarget=_top href="http://www.makelinux.com/kernel_map_poster?b"> <img target=_top src="http://www.makelinux.net/kernel_map.d/poster2.png" border=0></a>
</td></tr>
</table>
</center>
<script type='text/javascript' src='../common/fade.js'></script>
<script type=text/javascript>

	var banner = document.getElementById('banner');
	banner.style.backgroundPosition="50% 50%";
	banner.style.backgroundPosition=100*Math.random()+"% "+100*Math.random(100)+"%";
	//fade('banner');
    	//setTimeout("fade('banner')",1000);
    	//setTimeout("fade('banner2')",1000);
</script>

<script type="text/javascript">
var a = new Array();
a[0]='<a href=http://www.linuxdriver.co.il/>www.LinuxDriver.co.il - Embedded Linux solutions: Drivers, Media Streaming, Fast Boot. In Tel-Aviv</a>';
a[1]='<a href=http://www.MakeLinux.net/>www.MakeLinux.net - Embedded Linux solutions: Drivers, Media Streaming, Fast Boot</a>';
a[2]='<a href="http://www.amazon.com/gp/product/0672329468?ie=UTF8&tag=makelinux-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0672329468">New book <b>Linux Kernel Development</b> (3rd Edition) 2010</a><img src="http://www.assoc-amazon.com/e/ir?t=makelinux-20&l=as2&o=1&a=0672329468" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />';
a[3]='';
google_ad_width = 728;
google_ad_height = 1;
if ( 0) { 
document.write("<center>");
if (  geoip_country_code()=="IL" ) {
	if ( Math.random() > 0.5 ) document.write(a[0]+"<br>"); else document.write(a[2]);;
	google_ad_width = 728;
	google_ad_height = 1;
} else { 
	if ( Math.random() > 0.5 ) {
		if ( Math.random() > 0.5 ) document.write(a[1]+"<br>"); else document.write(a[2]);
		google_ad_width = 728;
		google_ad_height = 1;
	} else {
		google_ad_client = "pub-5656623102424572";
		/* 728x90, created 4/4/08 */
		google_ad_slot = "6613964975";
		google_ad_width = 728;
		google_ad_height = 100;
	}
}
	document.write("</center>");
}
</script>
<!--
<script type="text/javascript" src="http://pagead2.googlesyndication.com/pagead/show_ads.js"> </script>
-->



<script type="text/javascript" src="http://j.maxmind.com/app/geoip.js"></script>
<script type="text/javascript" >
{
	var os, br, ua = navigator.userAgent;

	if (ua.indexOf("Linux")!=-1) os="Linux";
	if (ua.indexOf("Windows")!=-1) os="Windows";
	if (ua.indexOf("Mac")!=-1) os="Mac";

	if (ua.indexOf("Gecko")!=-1) br="Gecko";
	if (ua.indexOf("Firefox")!=-1) br="Firefox";
	if (ua.indexOf("WebKit")!=-1) br="WebKit";
	if (ua.indexOf("MSIE")!=-1) br="MSIE";
	if (ua.indexOf("Safari")!=-1) br="Safari";
	if (ua.indexOf("Chrome")!=-1) br="Chrome";
	if (ua.indexOf("Konqueror")!=-1) br="Konqueror";
	if (ua.indexOf("Opera")!=-1) br="Opera";

	function query_var(query, variable) 
	{
		var vars = query.replace(/\?/g,"&").replace(/%20/g,"+").split("&");
		for (var i=0;i<vars.length;i++) {
			var pair = vars[i].split("=");
			if (pair[0] == variable) {
				return pair[1];
			}

		} 
	}
	var RQ="";
	q = query_var(top.document.referrer,'q');
	if ( q != undefined)
		RQ = ".&Q=" + q
	else if (top.document.referrer.length)
	
		RQ = ".&R=" + top.document.referrer;
	var traceimg= new Image();
	try {
		traceimg.src="http://const.homelinux.net/1.png?U="
			+ br + "-" + os 
			+"."+geoip_region_name().replace(/ /g,"_") 
			+"."+geoip_country_code()
			+ RQ ;
	} catch (e) {
	}
}
</script>


</body>
</html>
